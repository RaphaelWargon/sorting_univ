{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d25bd8-58c4-45aa-8c5a-1c9d97efc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run header_data_treatment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2c7df5-75b6-49d4-afab-12a96af85c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "works_au_af = spark.read.format('parquet').load('file:\\\\' + openalex_path + 'works_au_af.parquet')\n",
    "df_scanR = spark.read.format('parquet').load('file:\\\\' + scanR_path + 'publications.parquet')\n",
    "df_au_scanR = spark.read.format('parquet').load('file:\\\\' + scanR_path + 'authors.parquet')\n",
    "\n",
    "merge_path = 'D:\\\\openalex-snapshot\\\\merge\\\\'\n",
    "\n",
    "merge_pub = spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr.parquet')\n",
    "merge_authors = spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr_authors.parquet').withColumnRenamed('id','idref')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936fb64f-c00c-449b-8306-522f044b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_authors = merge_authors.withColumn('idref', func.regexp_replace(func.col('idref'), 'idref', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307d4435-e67b-4a29-aaa7-72221fc9ff94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expr_language ='CASE '+' '.join(\n",
    "    ['WHEN title.' + field + ' IS NOT NULL THEN \"'+ field + '\"' for field in df_scanR.select('title.*').columns if field != 'default']) + 'ELSE NULL  END'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f7cd2e-3109-4813-baf6-6ef5af5e4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = (spark.read.format('parquet').load('file:\\\\' + openalex_path + 'authors.parquet')\n",
    "              .withColumn('split_name', func.split(func.col('display_name'), ' '))\n",
    "             .withColumn('first_name', func.element_at(func.col('split_name'), 1))\n",
    "              .withColumn('last_name', func.concat_ws(' ', func.array_except(func.col('split_name'), func.array(func.col('first_name')))))\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea62bf81-5d28-4724-bd1f-500961a40236",
   "metadata": {},
   "outputs": [],
   "source": [
    "works_au_af = (works_au_af\n",
    "               .join(works_au_af\n",
    "                     .filter((func.col('country').isNull()) \n",
    "                             | (func.col('country')=='FR'))\n",
    "                     .select('work_id'), on = 'work_id', how = 'inner')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbd144-61c6-4d49-85ba-b1c03781f674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "works_au_af_w_names = (works_au_af\n",
    "                      .join(df_authors\n",
    "                      .withColumn('author_id', func.regexp_replace(func.col('id'), 'https://openalex.org/',''))\n",
    "                      .select('author_id', 'display_name', 'first_name', 'last_name'), on = ['author_id'], how = 'left')\n",
    "                      )\n",
    "works_au_af_w_names.cache()\n",
    "works_au_af_w_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34242a8e-1cc0-4c82-9a14-c317801ce9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scanR_exploded = (df_scanR\n",
    "                     .withColumn('language', func.expr(expr_language))\n",
    "                     .withColumn('authors', func.explode(func.col('authors')))\n",
    "                     .withColumn('idref', func.regexp_replace(func.lower(func.col(\"authors.person\")), 'idref',''))\n",
    "                     .withColumn('firstName', func.regexp_replace(func.col('authors.firstName'), '\\.', ''))\n",
    "                     .withColumn('lastName', func.regexp_replace(func.col('authors .lastName'), '\\.', ''))\n",
    "                     .withColumn('fullName', func.regexp_replace(func.col('authors.fullName'), '\\.', ''))\n",
    "                     .withColumn('initial', func.substring(func.col('firstName'),1,1))\n",
    "                     .withColumn('idref', func.when((func.col('idref').isNull()) & (func.col('productionType')!= 'thesis'), \n",
    "                                                func.col('fullName'))\n",
    "                            .when( (func.col('idref').isNull()) & (func.col('productionType')=='thesis'),\n",
    "                                                                    func.first(func.col('idref'), ignorenulls = True).over(Window.partitionBy('lastName','initial')))\n",
    "                             .otherwise(func.col('idref')))\n",
    "                     .withColumn('title', func.col('title.default'))\n",
    "                    )\n",
    "\n",
    "df_scanR_exploded.cache()\n",
    "df_scanR_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9aa17-14ab-49ca-9880-eb20516e73ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scanR.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec285ac5-3caa-4e05-b83c-91e2ab595020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "def unidecode_debug(x):\n",
    "    try:\n",
    "        y = unidecode.unidecode(x)\n",
    "        return(y)\n",
    "    except:\n",
    "        return(x)\n",
    "\n",
    "udf_unidecode = func.udf(unidecode_debug, StringType())\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())\n",
    "stopwords_dict = {}\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_dict[language.capitalize()] = stopwords.words(language)\n",
    "\n",
    "values = [(k, stopwords_dict[k]) for k in list(stopwords_dict.keys())]\n",
    "columns = ['language', 'stopwords']\n",
    "stopwords_df = spark.createDataFrame(values, columns)\n",
    "\n",
    "iso_639_choices ={\n",
    "'ab': 'Abkhaz', 'aa': 'Afar', 'af': 'Afrikaans', 'ak': 'Akan', 'sq': 'Albanian', 'am': 'Amharic', 'ar': 'Arabic',\n",
    "'an': 'Aragonese', 'hy': 'Armenian', 'as': 'Assamese', 'av': 'Avaric', 'ae': 'Avestan', 'ay': 'Aymara', 'az': 'Azerbaijani',\n",
    "'bm': 'Bambara', 'ba': 'Bashkir', 'eu': 'Basque', 'be': 'Belarusian', 'bn': 'Bengali', 'bh': 'Bihari', 'bi': 'Bislama',\n",
    "'bs': 'Bosnian', 'br': 'Breton', 'bg': 'Bulgarian', 'my': 'Burmese', 'ca': 'Catalan; Valencian', 'ch': 'Chamorro', 'ce': 'Chechen',\n",
    "'ny': 'Chichewa; Chewa; Nyanja', 'zh': 'Chinese', 'cv': 'Chuvash', 'kw': 'Cornish', 'co': 'Corsican', 'cr': 'Cree', 'hr': 'Croatian',\n",
    "'cs': 'Czech', 'da': 'Danish', 'dv': 'Divehi; Maldivian;', 'nl': 'Dutch', 'dz': 'Dzongkha', 'en': 'English', 'eo': 'Esperanto',\n",
    "'et': 'Estonian', 'ee': 'Ewe', 'fo': 'Faroese', 'fj': 'Fijian', 'fi': 'Finnish', 'fr': 'French', 'ff': 'Fula', 'gl': 'Galician',\n",
    "'ka': 'Georgian', 'de': 'German', 'el': 'Greek', 'gn': 'Guaraní', 'gu': 'Gujarati', 'ht': 'Haitian','ha': 'Hausa',\n",
    "'he': 'Hebrew','hz': 'Herero','hi': 'Hindi','hi': 'Hinglish','ho': 'Hiri Motu','hu': 'Hungarian','ia': 'Interlingua','id': 'Indonesian',\n",
    "'ie': 'Interlingue','ga': 'Irish','ig': 'Igbo','ik': 'Inupiaq','io': 'Ido','is': 'Icelandic','it': 'Italian','iu': 'Inuktitut',\n",
    "'ja': 'Japanese','jv': 'Javanese', 'kl': 'Kalaallisut', 'kn': 'Kannada', 'kr': 'Kanuri', 'ks': 'Kashmiri', \n",
    "'kk': 'Kazakh', 'km': 'Khmer', 'ki': 'Kikuyu, Gikuyu', 'rw': 'Kinyarwanda', 'ky': 'Kirghiz, Kyrgyz', 'kv': 'Komi', \n",
    "'kg': 'Kongo', 'ko': 'Korean', 'ku': 'Kurdish', 'kj': 'Kwanyama, Kuanyama', 'la': 'Latin', 'lb': 'Luxembourgish', \n",
    "'lg': 'Luganda', 'li': 'Limburgish', 'ln': 'Lingala', 'lo': 'Lao', 'lt': 'Lithuanian', 'lu': 'Luba-Katanga', 'lv': 'Latvian',\n",
    "'gv': 'Manx', 'mk': 'Macedonian', 'mg': 'Malagasy', 'ms': 'Malay', 'ml': 'Malayalam', 'mt': 'Maltese', 'mi': 'Māori', \n",
    "'mr': 'Marathi','mh': 'Marshallese','mn': 'Mongolian','na': 'Nauru','nv': 'Navajo','nb': 'Norwegian Bokmål',\n",
    "'nd': 'North Ndebele','ne': 'Nepali','ng': 'Ndonga','nn': 'Norwegian Nynorsk','no': 'Norwegian','ii': 'Nuosu',\n",
    "'nr': 'South Ndebele','oc': 'Occitan','oj': 'Ojibwe, Ojibwa','cu': 'Old Church Slavonic','om': 'Oromo','or': 'Oriya',\n",
    "'os': 'Ossetian, Ossetic','pa': 'Panjabi, Punjabi','pi': 'Pāli','fa': 'Persian','pl': 'Polish','ps': 'Pashto, Pushto',\n",
    "'pt': 'Portuguese','qu': 'Quechua','rm': 'Romansh','rn': 'Kirundi', 'ro': 'Romanian', 'ru': 'Russian', \n",
    "'sa': 'Sanskrit (Saṁskṛta)', 'sc': 'Sardinian', 'sd': 'Sindhi', 'se': 'Northern Sami', 'sm': 'Samoan', 'sg': 'Sango',\n",
    "'sr': 'Serbian', 'gd': 'Scottish Gaelic', 'sn': 'Shona', 'si': 'Sinhala, Sinhalese', 'sk': 'Slovak', 'sl': 'Slovene',\n",
    "'so': 'Somali', 'st': 'Southern Sotho', 'es': 'Spanish', 'su': 'Sundanese', 'sw': 'Swahili', 'ss': 'Swati', 'sv': 'Swedish', \n",
    "'ta': 'Tamil', 'te': 'Telugu', 'tg': 'Tajik', 'th': 'Thai', 'ti': 'Tigrinya','bo': 'Tibetan','tk': 'Turkmen','tl': 'Tagalog','tn': 'Tswana','to': 'Tonga','tr': 'Turkish','ts': 'Tsonga','tt': 'Tatar','tw': 'Twi','ty': 'Tahitian','ug': 'Uighur, Uyghur','uk': 'Ukrainian','ur': 'Urdu','uz': 'Uzbek','ve': 'Venda','vi': 'Vietnamese','vo': 'Volapük','wa': 'Walloon','cy': 'Welsh','wo': 'Wolof','fy': 'Western Frisian','xh': 'Xhosa','yi': 'Yiddish','yo': 'Yoruba','za': 'Zhuang, Chuang',\n",
    "'zu': 'Zulu', \"zh-cn\" : \"Chinese\"\n",
    "}\n",
    "        \n",
    "\n",
    "values = [(k, iso_639_choices[k]) for k in list(iso_639_choices.keys())]\n",
    "columns = [\"language_iso2\",'language']\n",
    "iso_df = spark.createDataFrame(values, columns)\n",
    "#iso_df.show()\n",
    "\n",
    "stopwords_df = (stopwords_df.join(iso_df.withColumn('language', func.explode(func.split(func.col('language'), '; '))), on = 'language', how = 'left')\n",
    "                .select('stopwords', func.col('language_iso2').alias('language'))\n",
    "               )\n",
    "def rm_stopwords(x,y):\n",
    "    if x is None:\n",
    "        return(x)\n",
    "    if y is None:\n",
    "        return(y)\n",
    "    else:\n",
    "        try:\n",
    "            list_x = x.split(' ')\n",
    "            list_y = y\n",
    "            return(' '.join([word for word in list_x if word not in list_y]))\n",
    "        except:\n",
    "            return(x)\n",
    "udf_rm_stopwords = func.udf(rm_stopwords)\n",
    "#stopwords_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b83148-d441-4f9b-94f1-b8ae06bea9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_unmatched(base_1, base_2, merged, id_1, id_2):\n",
    "\n",
    "    unmatched_1 = (base_1\n",
    "                   .join(merged.select(id_1), on = id_1, how = 'leftanti')\n",
    "                  )\n",
    "    unmatched_2 = (base_2\n",
    "                   .join(merged.select(id_2), on = id_2, how = 'leftanti')\n",
    "                  )\n",
    "    return(unmatched_1, unmatched_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a341b1-798f-4689-a826-c4db80a210cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_authors_from_pub(merged, base_1, base_2, pub_id_1, pub_id_2,\n",
    "                           au_id_1, au_id_2, last_name_1, last_name_2, name_1, name_2):\n",
    "\n",
    "    to_merge_1 = (base_1\n",
    "                .join(merged.select(pub_id_1), on = pub_id_1, how = 'inner')\n",
    "                .select(au_id_1, func.substring(func.col(name_1), 1, 1).alias('initial'), func.col(last_name_1).alias('last_name'), pub_id_1)\n",
    "               )\n",
    "    #print(to_merge_1.count())\n",
    "    to_merge_2 = (base_2\n",
    "                .join(merged.select(pub_id_1, pub_id_2), on = pub_id_2, how = 'inner')\n",
    "                .select(au_id_2, func.substring(func.col(name_2), 1, 1).alias('initial'), func.col(last_name_2).alias('last_name'), pub_id_1, pub_id_2)\n",
    "                 )\n",
    "    #print(to_merge_2.count())\n",
    "    matched = (to_merge_1\n",
    "               .join(to_merge_2, on = [pub_id_1, 'last_name', 'initial'], how = 'inner')\n",
    "               .withColumn('n_diff_id_1', func.size(func.collect_set(func.col(au_id_1)).over(Window.partitionBy(au_id_2))))\n",
    "               .withColumn('n_diff_id_2', func.size(func.collect_set(func.col(au_id_2)).over(Window.partitionBy(au_id_1))))\n",
    "               .filter( (func.col('n_diff_id_1')==1) &  (func.col('n_diff_id_2')==1))\n",
    "               .select(au_id_1, au_id_2)\n",
    "               .distinct()\n",
    "              )\n",
    "    return(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c8bad1-3e71-4808-8386-e532f3737d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pubs_from_authors(merged, base_1, base_2, pub_id_1, pub_id_2,\n",
    "                           au_id_1, au_id_2, title):\n",
    "\n",
    "    to_merge_1 = (base_1\n",
    "                .join(merged.select(au_id_1), on = au_id_1, how = 'inner')\n",
    "                .join(stopwords_df, on = ['language'], how = 'left')\n",
    "                .withColumn('cleaned_title', udf_rm_stopwords(func.lower(func.col(title)), func.col('stopwords')))\n",
    "                .withColumn('cleaned_title', udf_unidecode(func.col('cleaned_title')))\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), r'[[^A-Za-z0-9 -]]+', ''))  # Remove special characters\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), '  ', ''))  # Remove special characters\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), '  ', ''))  # Remove special characters\n",
    "                .select(au_id_1, 'cleaned_title', pub_id_1)\n",
    "               )\n",
    "    #print(to_merge_1.count())\n",
    "    to_merge_2 = (base_2\n",
    "                .join(merged.select(au_id_1, au_id_2), on = au_id_2, how = 'inner')\n",
    "                .join(stopwords_df, on = ['language'], how = 'left')\n",
    "                .withColumn('cleaned_title', udf_rm_stopwords(func.lower(func.col(title)), func.col('stopwords')))\n",
    "                .withColumn('cleaned_title', udf_unidecode(func.col('cleaned_title')))\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), r'[[^A-Za-z0-9 -]]+', ''))  # Remove special characters\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), '  ', ''))  # Remove special characters\n",
    "                .withColumn('cleaned_title', func.regexp_replace(func.col('cleaned_title'), '  ', ''))  # Remove special characters\n",
    "                .select(au_id_1, au_id_2, 'cleaned_title', pub_id_2)\n",
    "                 )\n",
    "    #print(to_merge_2.count())\n",
    "\n",
    "    matched = (to_merge_1\n",
    "               .join(to_merge_2, on = [au_id_1, 'cleaned_title'], how = 'inner')\n",
    "               .withColumn('n_diff_id_1', func.size(func.collect_set(func.col(pub_id_1)).over(Window.partitionBy(pub_id_2))))\n",
    "               .withColumn('n_diff_id_2', func.size(func.collect_set(func.col(pub_id_2)).over(Window.partitionBy(pub_id_1))))\n",
    "               .filter( (func.col('n_diff_id_1')==1) &  (func.col('n_diff_id_2')==1))\n",
    "               .select(pub_id_1, pub_id_2)\n",
    "               .distinct()\n",
    "              )\n",
    "    return(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45f56227-cbb3-40cc-a62a-5596160a5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    work_id|                  id|\n",
      "+-----------+--------------------+\n",
      "|W2912446109|doi10.1001/amajet...|\n",
      "|W2913698685|doi10.1001/jama.2...|\n",
      "|W2911290113|doi10.1001/jama.2...|\n",
      "|W2912431805|doi10.1001/jama.2...|\n",
      "|W2936759375|doi10.1001/jama.2...|\n",
      "|W2964642789|doi10.1001/jama.2...|\n",
      "|W2969751565|doi10.1001/jama.2...|\n",
      "|W2921906072|doi10.1001/jama.2...|\n",
      "|W2988464799|doi10.1001/jama.2...|\n",
      "|W2978101814|doi10.1001/jama.2...|\n",
      "|W2978470254|doi10.1001/jama.2...|\n",
      "|W2986540284|doi10.1001/jama.2...|\n",
      "|W2995672851|doi10.1001/jama.2...|\n",
      "|W2995170461|doi10.1001/jama.2...|\n",
      "|W2996126894|doi10.1001/jama.2...|\n",
      "|W2945858218|doi10.1001/jama.2...|\n",
      "|W2942311206|doi10.1001/jama.2...|\n",
      "|W2944420877|doi10.1001/jama.2...|\n",
      "|W2946375103|doi10.1001/jama.2...|\n",
      "|W2962938985|doi10.1001/jama.2...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_pub = spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr.parquet')\n",
    "merge_pub.select('work_id','id').unionAll(new_match_pubs_from_au).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c69f3fcc-8bbd-4a20-9594-9dbd7b82826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2\n",
      "Nr matches for publications:  201792 , nr matches for authors:  414106\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "iterations = 0\n",
    "while iterations < 1:\n",
    "    iterations = iterations +1\n",
    "    print('Iteration: ', iterations + 1)\n",
    "    if iterations == 1:\n",
    "        merge_pub = spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr.parquet')\n",
    "        merge_authors = (spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr_authors.parquet')\n",
    "                     .withColumnRenamed('id','idref')\n",
    "                     .withColumn('idref', func.regexp_replace(func.col('idref'), 'idref', '')))\n",
    "    else: \n",
    "        merge_pub = spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr_it.parquet')\n",
    "        merge_authors = (spark.read.format('parquet').load('file:\\\\' + merge_path + 'merge_oa_scanr_authors_it.parquet')\n",
    "                     .withColumnRenamed('id','idref')\n",
    "                     .withColumn('idref', func.regexp_replace(func.col('idref'), 'idref', '')))\n",
    "\n",
    "    print('Nr matches for publications: ', merge_pub.count(), ', nr matches for authors: ',    merge_authors.count())\n",
    "\n",
    "    \n",
    "    unmatched_pubs_oa, unmatched_pubs_scanR = get_only_unmatched(works_au_af_w_names, df_scanR_exploded, merge_pub, 'work_id', 'id')\n",
    "    unmatched_au_oa, unmatched_au_scanR = get_only_unmatched(works_au_af_w_names, df_scanR_exploded, merge_authors, 'author_id', 'idref')\n",
    "\n",
    "    print('ok')\n",
    "    new_match_au_from_pubs = match_authors_from_pub(merge_pub, unmatched_au_oa, unmatched_au_scanR, 'work_id', 'id',\n",
    "                           'author_id', 'idref', 'last_name', 'lastName', 'display_name', 'fullName')\n",
    "    \n",
    "    new_match_pubs_from_au = match_pubs_from_authors(merge_authors, unmatched_pubs_oa, unmatched_pubs_scanR, 'work_id', 'id',\n",
    "                           'author_id', 'idref', 'title')\n",
    "\n",
    "    merge_authors = (merge_authors.unionAll(new_match_au_from_pubs))\n",
    "    merge_pub = (merge_pub.select('work_id', 'id').unionAll(new_match_pubs_from_au))\n",
    "\n",
    "    merge_authors.count()\n",
    "    merge_pub.count()\n",
    "    merged_authors.write.mode('overwrite').parquet('file:\\\\' + merge_path + 'merge_oa_scanr_authors_it.parquet')\n",
    "    merge_pub.write.mode('overwrite').parquet('file:\\\\' + merge_path + 'merge_oa_scanr_it.parquet')\n",
    "    #print('New matches of authors from publications : ', new_match_au_from_pubs.count(), '\\n',\n",
    "    #      'New matches of publications from authors : ', new_match_pubs_from_au.count())\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "227cc382-bce3-40a9-911f-1690976b4fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    work_id|                  id|\n",
      "+-----------+--------------------+\n",
      "|W1000137256|   halinria-00596868|\n",
      "| W100014352|     nnt2003aix20651|\n",
      "| W100015661|     nnt2009lyo10148|\n",
      "| W100021887|     nnt2004mon20145|\n",
      "|W1000261152|doi10.1016/j.msea...|\n",
      "|W1000436346|  halhalshs-00777257|\n",
      "|  W10006723|     nnt2011clf22104|\n",
      "| W100076178|     nnt2010tou30278|\n",
      "|W1000816400|doi10.1007/s13171...|\n",
      "| W100088650|     nnt2009pa066033|\n",
      "| W100093082|     nnt1999toul0006|\n",
      "|W1000960367|   halinria-00549216|\n",
      "|W1001064943|     halhal-01647440|\n",
      "|W1001077819|doi10.1103/physre...|\n",
      "|W1001091333|     nnt2013aixm4077|\n",
      "|W1001093056|  halhalshs-00637916|\n",
      "| W100110499|     nnt1987dijos036|\n",
      "| W100113257|     nnt2011pa100093|\n",
      "| W100113666|     nnt2004bor30070|\n",
      "| W100162093|     halhal-00809316|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_match_pubs_from_au.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "300cfeb5-a166-4ee4-b064-8211a3c688b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pub_new = merge_pub.select('work_id','id').unionAll(new_match_pubs_from_au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ddaddb9-1bc1-495f-b9ee-2503ee8803b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\rapha\\anaconda3\\Lib\\socket.py\", line 709, in readinto\n    raise\nTimeoutError: timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merge_pub_new\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\rapha\\anaconda3\\Lib\\socket.py\", line 709, in readinto\n    raise\nTimeoutError: timed out\n"
     ]
    }
   ],
   "source": [
    "merge_pub_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4155810f-e812-400a-8b16-92239949809c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3668.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 99 in stage 283.0 failed 1 times, most recent failure: Lost task 99.0 in stage 283.0 (TID 3820) (localhost executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:217)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:217)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNr matches for publications: \u001b[39m\u001b[38;5;124m'\u001b[39m, merge_pub\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:1234\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3668.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 99 in stage 283.0 failed 1 times, most recent failure: Lost task 99.0 in stage 283.0 (TID 3820) (localhost executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:217)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:217)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "print('Nr matches for publications: ', merge_pub.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e19a85d-2b33-4623-9719-072ae40129fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         idref|\n",
      "+--------------+\n",
      "|idref108538427|\n",
      "|idref225993244|\n",
      "|idref17820806X|\n",
      "|idref097879134|\n",
      "|idref079013848|\n",
      "|idref27199214X|\n",
      "|idref117817007|\n",
      "|idref258447958|\n",
      "|idref140407960|\n",
      "|idref221650709|\n",
      "|idref081786883|\n",
      "|idref078649781|\n",
      "|idref178968315|\n",
      "|idref250398206|\n",
      "|idref25101651X|\n",
      "|idref165190132|\n",
      "|idref175449716|\n",
      "|idref232567670|\n",
      "|idref116414014|\n",
      "|idref196687144|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19af5570-e847-4e27-b7e5-ae3106c97b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               idref|\n",
      "+--------------------+\n",
      "|Jonathan A Christ...|\n",
      "| Raymond A Ezejiofor|\n",
      "|   A K Rakhmetullina|\n",
      "|     Hatem A Rashwan|\n",
      "|     Larissa A Rolim|\n",
      "|     Habibah A Wahab|\n",
      "|   Gabriel AB Marais|\n",
      "|                NULL|\n",
      "|    AHMAD ABD‐ALAZIZ|\n",
      "|           124735150|\n",
      "|                NULL|\n",
      "|             A ADDED|\n",
      "|        NEZAM AFDHAL|\n",
      "|             D AGIUS|\n",
      "|           178752061|\n",
      "|                NULL|\n",
      "|                NULL|\n",
      "|                NULL|\n",
      "|           086036904|\n",
      "|           086036904|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scanR_exploded.select('idref').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4f18fe-55ad-43a0-89b0-71f6fbf8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_pubs_oa, unmatched_pubs_scanR = get_only_unmatched(works_au_af_w_names, df_scanR_exploded, merge_pub, 'work_id', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30aec38-d21b-4297-a62c-42013e3aa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_au_oa, unmatched_au_scanR = get_only_unmatched(works_au_af_w_names, df_scanR_exploded, merge_authors, 'author_id', 'idref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899fa06a-d0be-415b-9002-b1b3d0a7571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38338654\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[work_id: string, id: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_pubs_from_authors(merge_authors, unmatched_pubs_oa, unmatched_pubs_scanR, 'work_id', 'id',\n",
    "                           'author_id', 'idref', 'title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8734f3d9-f607-474d-844f-5976011c3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38338654\n",
      "0\n",
      "+-------+---+\n",
      "|work_id| id|\n",
      "+-------+---+\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_pubs_from_authors(merge_authors, unmatched_pubs_oa, unmatched_pubs_scanR, 'work_id', 'id',\n",
    "                           'author_id', 'idref', 'title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b1c0039-d66f-48a7-9292-a729284ddce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|         idref|  author_id|\n",
      "+--------------+-----------+\n",
      "|idref108538427|A5080431922|\n",
      "|idref225993244|A5009476002|\n",
      "|idref17820806X|A5045317380|\n",
      "|idref097879134|A5043440547|\n",
      "|idref079013848|A5091569743|\n",
      "|idref27199214X|A5073550025|\n",
      "|idref117817007|A5079720434|\n",
      "|idref258447958|A5018502157|\n",
      "|idref140407960|A5007698196|\n",
      "|idref221650709|A5018830374|\n",
      "|idref081786883|A5040388573|\n",
      "|idref078649781|A5065112242|\n",
      "|idref178968315|A5005892790|\n",
      "|idref250398206|A5078787536|\n",
      "|idref25101651X|A5072381169|\n",
      "|idref165190132|A5028531834|\n",
      "|idref175449716|A5009278937|\n",
      "|idref232567670|A5079074755|\n",
      "|idref116414014|A5002320268|\n",
      "|idref196687144|A5023926454|\n",
      "+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_authors[].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97623fa5-ec46-457b-a160-1515ac24ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               idref|\n",
      "+--------------------+\n",
      "|076086224###g.maz...|\n",
      "|189607939###ouria...|\n",
      "|           Alice M 1|\n",
      "|Achille Cesare Pe...|\n",
      "|           Geloen 12|\n",
      "|Holger S Willenbe...|\n",
      "|             Huot 14|\n",
      "|Vila 1exmulri Mag...|\n",
      "|            342 Choi|\n",
      "|       457 Borgonovo|\n",
      "|  Leo Schultzekool 7|\n",
      "|145470768###aurél...|\n",
      "|076926656###aurél...|\n",
      "|169207226###renau...|\n",
      "|13028906x###tara ...|\n",
      "|13028906x###tara ...|\n",
      "|13028906x###tara ...|\n",
      "|13028906x###tara ...|\n",
      "|13028906x###tara ...|\n",
      "|13028906x###tara ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scanR_exploded.select('idref').filter(func.col('idref').rlike('\\d')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa2d7618-e8d7-4471-8189-34ad3c7554a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+--------------------+\n",
      "|               idref|initial|     last_name|                  id|\n",
      "+--------------------+-------+--------------+--------------------+\n",
      "|  Alain Palloix ( ‡)|      A|          ( ‡)|doi10.25518/1780-...|\n",
      "|              Goel A|      G|             A|doi10.16965/ijar....|\n",
      "|            Gevrey A|      G|             A|     halhal-02304673|\n",
      "|            Gevrey A|      G|             A|     halhal-02304776|\n",
      "|           Garnier A|      G|             A|     halhal-03566942|\n",
      "|          Harroche A|      H|             A|doi10.31031/rpn.2...|\n",
      "|         Houeijeh, A|      H|             A|     halhal-02466637|\n",
      "|           Lemaire A|      L|             A|doi10.16966/2378-...|\n",
      "|     Leitenstorfer A|      L|             A|doi10.1051/epjcon...|\n",
      "|  Lacorte-Bruguera A|      L|             A|     halhal-02332778|\n",
      "|      A Myrkassimova|      A|A Myrkassimova|doi10.32014/2019....|\n",
      "|     Larissa A Rolim|      L|       A Rolim|doi10.21577/1984-...|\n",
      "|     Habibah A Wahab|      H|       A Wahab|doi10.21315/jps20...|\n",
      "|Mohammed ABDUL SAMAD|      M|   ABDUL SAMAD|doi10.1016/s1003-...|\n",
      "|     AYMAN S ABUTAIR|      A|       ABUTAIR|doi10.12944/crnfs...|\n",
      "|        NEZAM AFDHAL|      N|        AFDHAL|doi10.2337/db19-1...|\n",
      "|199566828###achal...|      A|       AGRAWAL|doi10.1109/wimob....|\n",
      "|      GROUP AGRICAN-|      G|      AGRICAN-|doi10.1136/oem-20...|\n",
      "|     Oussama AHRAZEM|      O|       AHRAZEM|doi10.3906/bot-18...|\n",
      "|FLAVIO DE ALMEIDA...|      F|  ALVES-JÚNIOR|doi10.11646/zoota...|\n",
      "+--------------------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(unmatched_au_scanR\n",
    "                .join(merge_pub.select('work_id', 'id'), on = 'id', how = 'inner')\n",
    "                .select('idref', func.substring(func.col('fullName'), 1, 1).alias('initial'), func.col('lastName').alias('last_name'), 'id','work_id')\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e1e6e-f129-49f4-af48-9ca1788bc86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
